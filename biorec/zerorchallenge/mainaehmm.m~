% MAINAEHMM Trains a HMM-Encoder to discover sub-words units from the
% untrascribed utterances. Given an input speech dataset 
% it output the sub-word transcription of each input utterance in the 
% Zero Resource Speech challenge format 

clear all;
close all;

% Hyper-parameters of the autoencoder. See ae_hyper-param_example.m for more details 
parnet = struct(...
    'units',[60 100 50 6],...   % input + hidden (last is the encoding layer)
    'activations',{{'sigm','sigm','sigm'}},... % hidden
    'pretrain','rbm',...    % rbm/ae/rand
    'preae_maxepoch',50,... % for 'ae' pretraining, number of epochs for each layer
    'vgaus',1,...   % for 'rbm' pretraining, gaussian visible units
    'hgaus',0,...   % for 'rbm' pretraining, gaussian hidden units
    'pairsim',0,... % SAE using similarity heuristic
    'shid',0,...    % SCAE using similarity heuristic
    'noisy',0,...   % DAE
    'binarize',0,...    % binarize encodings in the fwd pass: 1) threshold 0.5; 2) threshold stochastically
    'forceBin',0,...    % force binarization: 1) add noise to encoding layer input; 2) additional entropy cost term
    'noiseVariance',16,...  % for forceBin=1, variance of the noise
    'entropyWeight',1,...   % for forceBin=2, weight of the entropy term
    'sparsepen',0,...   % sparsity penalty (beta)
    'sparsepar',0.05,...    % sparsity parameter (rho)
    'batchsize',1000,...
    'maxepoch',10,...
    'optimization','cgd',...    % cgd/sgd
    'cost','mse',...
    'dropout',0,...
    'learningRate',0.1,...          % SGD parameters
    'learningRateDecay',0.99,...    %%
    'momentum',0.9,...              %%
    'weightDecay',0.0001,...
    'rbmparam',struct(...           
        'vgaus',0,...
        'hgaus',0,...
        'batchsize',50,...
        'maxepoch',10,...
        'epsilonw',0.1,...
        'epsilonvb',0.1,...
        'epsilonhb',0.1,...
        'weightcost',0.0002,...
        'initialmomentum',0.5,...
        'finalmomentum',0.9...
        ),...
    'rbmhgausparam',struct(...
        'vgaus',0,...
        'hgaus',1,...
        'batchsize',50,...
        'maxepoch',20,...
        'epsilonw',0.001,...
        'epsilonvb',0.001,...
        'epsilonhb',0.001,...
        'weightcost',0.0002,...
        'initialmomentum',0.5,...
        'finalmomentum',0.9...
        ),...
    'rbmvgausparam',struct(...
        'vgaus',1,...
        'hgaus',0,...
        'batchsize',50,...
        'maxepoch',20,...
        'epsilonw',0.001,...
        'epsilonvb',0.001,...
        'epsilonhb',0.001,...
        'weightcost',0.0002,...
        'initialmomentum',0.5,...
        'finalmomentum',0.9...
        )...
);

% defines the way subword posterior probabilities are extracted. Keep mode
% = enc
mode = 'enc';
% maximum number of EM iterations for HMM-Encoder training
maxiter = 2;

% input file 
filedata = 'sampleDataSINGLE';
% file where the autoencoder will be saved or from where the autoencoder
% has to be loaded
filenet = [];

% file where the hmm-encoder will be saved
aehfile = [];

%directory where outputfile will be written
outdir1 = [];
%outdir2 = [];

% Q = number of sub-words to discover
Q = 2.^parnet.units(end);

mkdir(outdir1);
%mkdir(outdir2);
load(filedata);
if ~isempty(filenet)
    load(filenet);
    parnet = prevhae.parnet;
    %train the hmm-encoder
    [hae, prevhae, loglik, storehae] = hmmauto_learn(traindata,parnet,wordTrainStart,wordTrainEnd,testdata,mode,'maxiter',maxiter,'net',prevhae.net);
else
    %train the hmm-encoder
    [hae, prevhae, loglik, storehae] = hmmauto_learn(traindata,parnet,wordTrainStart,wordTrainEnd,testdata,mode,'maxiter',maxiter);
end

save(aehfile,'hae','parnet','prevhae','loglik','storehae');
ntrutts = length(wordTrainStart);
ntsutts = length(wordTestStart);
Nu = ntrutts + ntsutts;
utt.data = cell(1,Nu);
utt.name = cell(1,Nu);

alldata = [traindata;testdata];
allStart = zeros(1,Nu);
allEnd = zeros(1,Nu);
for i=1:ntrutts        
    utt.data{i} = traindata(wordTrainStart(i):wordTrainEnd(i),:);
    utt.name{i} = wordTrainName{i};
end

for i=1:ntsutts
    utt.data{i+ntrutts} = testdata(wordTestStart(i):wordTestEnd(i),:);  
    utt.name{i+ntrutts} = wordTestName{i};
end

allStart(1:ntrutts) = wordTrainStart;
allEnd(1:ntrutts) = wordTrainEnd;
allStart(ntrutts+1:end) = wordTestStart + size(traindata,1);
allEnd(ntrutts+1:end) = wordTestEnd + size(traindata,1);

% decode each utterance providing the sequence of discrete subwords undelrying the
% utterance. subwords here are represented in the 1-of-K format (aka 1 hot
% representation)
utt.encoded1oK = hmmauto_decode(hae,alldata,allStart,allEnd,mode);

len = 0;
for i=1:Nu
    len = len+length(rmRepInarow(utt.encoded1oK{i}));
end
alen = len./Nu;
printoutaeh_zr(outdir1,utt.name,utt.encoded1oK,Q);


% nohmmutts = hmmauto_decode(prevhae,alldata,allStart,allEnd,mode);
% 
% len = 0;
% for i=1:Nu
%     len = len+length(rmRepInarow(nohmmutts{i}));
% end
% alen_nohmm = len./Nu;
% 
% printoutaeh_zr(outdir2,utt.name,nohmmutts,Q);
